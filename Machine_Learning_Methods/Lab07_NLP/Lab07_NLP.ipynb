{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download \"Alice in Wonderland\" by Lewis Carroll from Gutenberg project http://www.gutenberg.org/files/11/11-0.txt\n",
    "2. Perform all the necessary preprocessing, including lowercasing, removing stopwords, numbers/non-alhabhetic symbols, etc.\n",
    "3. Find Top-10 most important (in terms of count vectorizer or TF-IDF, for example) words from every chapter in the text (not \"Alice\"); how could you name each chapter according to the evaluated tokens?\n",
    "4. Find the Top-10 most frequently used verbs in sentences with Alice. What does Alice do most often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'alice.txt'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "chapters = text.split('CHAPTER')\n",
    "# delete introcudtion\n",
    "chapters = chapters[1:]\n",
    "# delete text after 'THE END'\n",
    "last_chapter = chapters[-1]\n",
    "last_chapter = last_chapter.split('THE END')\n",
    "chapters[-1] = last_chapter[0]\n",
    "print(len(chapters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if not word in stop_words]\n",
    "    tokens = [WordNetLemmatizer().lemmatize(word, 'v') for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = [clean_text(chapter) for chapter in chapters]\n",
    "#join words in chapter for CountVectorizer()\n",
    "chapters = [(' ').join(chapter) for chapter in chapters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-10 most important words in every chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Most important wrods for 1 chapter\n",
      "('think', 19)\n",
      "('say', 16)\n",
      "('go', 15)\n",
      "('little', 15)\n",
      "('get', 14)\n",
      "('see', 13)\n",
      "('find', 11)\n",
      "('like', 11)\n",
      "('way', 11)\n",
      "('eat', 10)\n",
      "10 Most important wrods for 2 chapter\n",
      "('go', 21)\n",
      "('mouse', 20)\n",
      "('say', 19)\n",
      "('little', 17)\n",
      "('think', 14)\n",
      "('oh', 13)\n",
      "('come', 10)\n",
      "('cry', 10)\n",
      "('dear', 10)\n",
      "('like', 9)\n",
      "10 Most important wrods for 3 chapter\n",
      "('say', 40)\n",
      "('mouse', 20)\n",
      "('know', 13)\n",
      "('dodo', 12)\n",
      "('get', 9)\n",
      "('think', 8)\n",
      "('find', 7)\n",
      "('one', 7)\n",
      "('soon', 7)\n",
      "('bird', 6)\n",
      "10 Most important wrods for 4 chapter\n",
      "('little', 23)\n",
      "('say', 19)\n",
      "('go', 17)\n",
      "('get', 16)\n",
      "('come', 15)\n",
      "('one', 15)\n",
      "('rabbit', 15)\n",
      "('bill', 14)\n",
      "('make', 14)\n",
      "('grow', 13)\n",
      "10 Most important wrods for 5 chapter\n",
      "('say', 57)\n",
      "('caterpillar', 26)\n",
      "('think', 13)\n",
      "('pigeon', 12)\n",
      "('get', 11)\n",
      "('little', 11)\n",
      "('well', 10)\n",
      "('serpent', 9)\n",
      "('try', 9)\n",
      "('begin', 8)\n",
      "10 Most important wrods for 6 chapter\n",
      "('say', 53)\n",
      "('cat', 24)\n",
      "('go', 23)\n",
      "('think', 17)\n",
      "('like', 16)\n",
      "('duchess', 14)\n",
      "('little', 14)\n",
      "('get', 13)\n",
      "('know', 13)\n",
      "('baby', 12)\n",
      "10 Most important wrods for 7 chapter\n",
      "('say', 70)\n",
      "('hatter', 33)\n",
      "('dormouse', 27)\n",
      "('march', 22)\n",
      "('hare', 21)\n",
      "('go', 20)\n",
      "('time', 17)\n",
      "('know', 15)\n",
      "('think', 12)\n",
      "('well', 12)\n",
      "10 Most important wrods for 8 chapter\n",
      "('say', 47)\n",
      "('queen', 37)\n",
      "('go', 24)\n",
      "('look', 21)\n",
      "('head', 16)\n",
      "('come', 14)\n",
      "('king', 13)\n",
      "('begin', 12)\n",
      "('get', 12)\n",
      "('cat', 11)\n",
      "10 Most important wrods for 9 chapter\n",
      "('say', 68)\n",
      "('turtle', 28)\n",
      "('mock', 26)\n",
      "('go', 20)\n",
      "('gryphon', 20)\n",
      "('duchess', 19)\n",
      "('queen', 14)\n",
      "('think', 14)\n",
      "('make', 11)\n",
      "('never', 11)\n",
      "10 Most important wrods for 10 chapter\n",
      "('say', 50)\n",
      "('gryphon', 31)\n",
      "('turtle', 31)\n",
      "('mock', 28)\n",
      "('would', 16)\n",
      "('dance', 15)\n",
      "('go', 14)\n",
      "('come', 12)\n",
      "('soup', 12)\n",
      "('beautiful', 11)\n",
      "10 Most important wrods for 11 chapter\n",
      "('say', 42)\n",
      "('king', 26)\n",
      "('hatter', 21)\n",
      "('court', 15)\n",
      "('begin', 13)\n",
      "('dormouse', 13)\n",
      "('look', 12)\n",
      "('one', 11)\n",
      "('get', 10)\n",
      "('witness', 10)\n",
      "10 Most important wrods for 12 chapter\n",
      "('say', 51)\n",
      "('king', 22)\n",
      "('would', 13)\n",
      "('know', 11)\n",
      "('jury', 10)\n",
      "('little', 10)\n",
      "('queen', 10)\n",
      "('write', 10)\n",
      "('look', 9)\n",
      "('dream', 8)\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(chapters)):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer_fit = vectorizer.fit_transform(chapters[j].split(' '))\n",
    "    cur_names = vectorizer.get_feature_names()\n",
    "    freq = vectorizer_fit.toarray().sum(axis = 0)\n",
    "    cur_dict = {cur_names[i]:freq[i] for i in range(len(cur_names))}\n",
    "    best_list = sorted(list(cur_dict.items()), key = lambda x: x[1], reverse = True)\n",
    "    best_list = [item for item in best_list if item[0] != 'alice']\n",
    "    print('10 Most important wrods for ' + str(j + 1) + ' chapter')\n",
    "    for item in best_list[:10]:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-10 most frequently words in sentence with Alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('go', 51), ('get', 40), ('say', 39), ('think', 23), ('take', 22), ('see', 18), ('know', 13), ('keep', 13), ('tell', 13), ('find', 12)]\n"
     ]
    }
   ],
   "source": [
    "result_words = Counter()\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = clean_text(sentence)\n",
    "    if 'alice' in sentence:\n",
    "        sentence_words = nltk.pos_tag(sentence)\n",
    "        for word, pos_tag in sentence_words:\n",
    "            if pos_tag == 'VB':\n",
    "                result_words[word] += 1\n",
    "                \n",
    "print(result_words.most_common(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
